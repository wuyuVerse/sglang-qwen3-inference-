# SGLang Qwen3 æ¨ç†æœåŠ¡

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![SGLang](https://img.shields.io/badge/SGLang-0.4.0%2B-green.svg)](https://github.com/sgl-project/sglang)
[![License](https://img.shields.io/badge/License-Apache_2.0-yellow.svg)](https://opensource.org/licenses/Apache-2.0)

åŸºäº [SGLang](https://github.com/sgl-project/sglang) çš„ Qwen3 æ¨¡å‹é«˜æ€§èƒ½æ¨ç†æœåŠ¡

</div>

## ğŸš€ é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªåŸºäº SGLang æ¡†æ¶çš„ Qwen3 æ¨¡å‹æ¨ç†æœåŠ¡é¡¹ç›®ï¼Œæä¾›äº†å®Œæ•´çš„æ¨¡å‹éƒ¨ç½²ã€é‡åŒ–ä¼˜åŒ–å’ŒAPIæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚

### âœ¨ ä¸»è¦ç‰¹æ€§

- **ğŸ”¥ é«˜æ€§èƒ½æ¨ç†**ï¼šåŸºäº SGLang çš„ä¼˜åŒ–åç«¯ï¼Œæ”¯æŒ RadixAttentionã€é›¶å¼€é”€è°ƒåº¦ç­‰åŠ é€ŸæŠ€æœ¯
- **ğŸ’¾ æ™ºèƒ½é‡åŒ–**ï¼šæ”¯æŒå¤šç§é‡åŒ–æ–¹æ¡ˆï¼ˆINT4/INT8/FP8ï¼‰ï¼Œé€‚é…ä¸åŒç¡¬ä»¶é…ç½®
- **ğŸŒ æ ‡å‡†API**ï¼šå…¼å®¹ OpenAI API æ ¼å¼ï¼Œæ— ç¼é›†æˆç°æœ‰åº”ç”¨
- **âš™ï¸ çµæ´»é…ç½®**ï¼šYAMLé…ç½®æ–‡ä»¶ + å‘½ä»¤è¡Œå‚æ•°ï¼Œæ”¯æŒé¢„è®¾å’Œè‡ªå®šä¹‰é…ç½®
- **ğŸ“± å¤šç§æ¥å£**ï¼šæ”¯æŒ HTTP APIã€Python SDKã€SGLang å‰ç«¯è¯­è¨€
- **ğŸ”§ æ˜“äºéƒ¨ç½²**ï¼šå®Œæ•´çš„å¯åŠ¨è„šæœ¬å’Œæµ‹è¯•å·¥å…·ï¼Œå¼€ç®±å³ç”¨

### ğŸ¯ æ”¯æŒçš„æ¨¡å‹

- Qwen3-4B / Qwen3-14B
- å…¶ä»– SGLang æ”¯æŒçš„æ¨¡å‹ï¼ˆLlamaã€Gemmaã€Mistral ç­‰ï¼‰

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚

| æ¨¡å‹ | æœ€å°æ˜¾å­˜ | æ¨èæ˜¾å­˜ | æ¨èé…ç½® |
|------|----------|----------|----------|
| Qwen3-4B | 6GB | 12GB | RTX 3080+ |
| Qwen3-14B | 12GB | 28GB | RTX 4090+ |

### è½¯ä»¶è¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Linux (æ¨è Ubuntu 20.04+)
- **Python**: 3.8+
- **CUDA**: 11.8+ æˆ– 12.0+
- **GPU**: NVIDIA GPU (æ”¯æŒ CUDA)

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/wuyuVerse/sglang-qwen3-inference-.git
cd sglang-qwen3-inference
```

### 2. å®‰è£…ä¾èµ–

#### æ–¹æ³• A: ä½¿ç”¨ uv (æ¨è)

```bash
# å®‰è£… uv (å¦‚æœæœªå®‰è£…)
curl -LsSf https://astral.sh/uv/install.sh | sh

# å®‰è£…é¡¹ç›®ä¾èµ–
uv sync

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate
```

#### æ–¹æ³• B: ä½¿ç”¨ pip

```bash
pip install -r requirements.txt
# æˆ–è€…æ‰‹åŠ¨å®‰è£…æ ¸å¿ƒä¾èµ–
pip install sglang[all] torch transformers accelerate fastapi uvicorn
```

### 3. å¯åŠ¨æœåŠ¡

#### ä¸€é”®å¯åŠ¨ (æ¨è)

```bash
# å¹³è¡¡é…ç½® - é€‚åˆå¤§å¤šæ•°åœºæ™¯
python launch_server.py --preset balanced

# é«˜æ€§èƒ½é…ç½® - é€‚åˆé«˜ç«¯GPU
python launch_server.py --preset high_performance

# å†…å­˜ä¼˜åŒ–é…ç½® - é€‚åˆæ˜¾å­˜ä¸è¶³
python launch_server.py --preset memory_optimized
```

#### è‡ªå®šä¹‰å¯åŠ¨

```bash
# ä½¿ç”¨ TorchAO INT4 é‡åŒ–
python launch_server.py --torchao-config int4wo-64

# æŒ‡å®šæ¨¡å‹è·¯å¾„
python launch_server.py --model-path /path/to/qwen3-model --preset balanced

# å¤šGPUéƒ¨ç½²
python launch_server.py --preset balanced --tp-size 2
```

### 4. æµ‹è¯•æœåŠ¡

```bash
# åŸºç¡€è¿æ¥æµ‹è¯•
python test_client.py

# è‡ªå®šä¹‰æµ‹è¯•
python test_client.py --prompt "ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ " --max-tokens 200
```

## ğŸ“š è¯¦ç»†ä½¿ç”¨æŒ‡å—

### ğŸ›ï¸ é…ç½®é€‰æ‹©æŒ‡å—

#### é¢„è®¾é…ç½®å¯¹æ¯”

| é¢„è®¾ | é‡åŒ–æ–¹æ³• | å†…å­˜èŠ‚çœ | æ¨ç†é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|----------|----------|
| `high_performance` | æ— é‡åŒ– | 0% | 100% | é«˜ç«¯GPUï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½ |
| `balanced` | INT4 | ~50% | 85-95% | **æ¨èé…ç½®**ï¼Œæ€§èƒ½ä¸å†…å­˜å¹³è¡¡ |
| `memory_optimized` | INT4+ä¼˜åŒ– | ~60% | 75-85% | æ˜¾å­˜ä¸è¶³ï¼Œä¼˜å…ˆå¯ç”¨æ€§ |
| `ultra_low_memory` | INT8 | ~75% | 60-75% | ä½ç«¯GPUï¼Œæœ€å¤§å…¼å®¹æ€§ |

#### ç¡¬ä»¶é…ç½®å»ºè®®

```bash
# RTX 3060 (8GB)
python launch_server.py --preset ultra_low_memory --model-path /path/to/qwen3-4b

# RTX 3080/4080 (10-16GB)  
python launch_server.py --preset memory_optimized

# RTX 4090 (24GB)
python launch_server.py --preset balanced

# A100/H100 (40GB+)
python launch_server.py --preset high_performance
```

### ğŸ”§ é«˜çº§é…ç½®

#### ä½¿ç”¨é…ç½®æ–‡ä»¶

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶
python launch_server.py --config server_config.yaml

# ä½¿ç”¨ç¤ºä¾‹é…ç½®æ–‡ä»¶
python launch_server.py --config config_examples.yaml

# é…ç½®æ–‡ä»¶ + å‘½ä»¤è¡Œè¦†ç›–
python launch_server.py --config server_config.yaml --torchao-config int4wo-128
```

#### TorchAO é‡åŒ–é…ç½®

```bash
# INT4 æƒé‡é‡åŒ– (æ¨è)
python launch_server.py --torchao-config int4wo-64   # æ›´å¿«
python launch_server.py --torchao-config int4wo-128  # æ›´å‡†ç¡®

# INT8 é‡åŒ–
python launch_server.py --torchao-config int8wo

# FP8 é‡åŒ– (éœ€è¦ H100/H200)
python launch_server.py --torchao-config fp8dq-per_tensor
```

### ğŸŒ API ä½¿ç”¨

#### HTTP API

**Completions API**

```bash
curl -X POST "http://localhost:30000/v1/completions" \
     -H "Content-Type: application/json" \
     -d '{
       "model": "default",
       "prompt": "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹",
       "max_tokens": 200,
       "temperature": 0.7
     }'
```

**Chat Completions API**

```bash
curl -X POST "http://localhost:30000/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -d '{
       "model": "default",
       "messages": [
         {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
       ],
       "max_tokens": 200,
       "temperature": 0.7
     }'
```

#### Python SDK

```python
import requests

# ç®€å•å¯¹è¯
response = requests.post("http://localhost:30000/v1/chat/completions", 
    json={
        "model": "default",
        "messages": [{"role": "user", "content": "ä½ å¥½"}],
        "max_tokens": 100
    }
)
print(response.json()["choices"][0]["message"]["content"])
```

#### SGLang å‰ç«¯è¯­è¨€

```python
# è¿è¡Œé«˜çº§ç¤ºä¾‹
python sglang_example.py

# æˆ–è€…è‡ªå®šä¹‰ä½¿ç”¨
import sglang as sgl

sgl.set_default_backend(sgl.RuntimeEndpoint("http://localhost:30000"))

@sgl.function
def simple_chat(s, question):
    s += sgl.user(question)
    s += sgl.assistant(sgl.gen("answer", max_tokens=100))

state = simple_chat.run(question="ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
print(state["answer"])
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
sglang-qwen3-inference/
â”œâ”€â”€ ğŸ“„ README.md                    # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ ğŸ“„ pyproject.toml               # é¡¹ç›®é…ç½®å’Œä¾èµ–
â”œâ”€â”€ ğŸ“„ uv.lock                      # ä¾èµ–é”å®šæ–‡ä»¶
â”œâ”€â”€ ğŸ“„ .gitignore                   # Gitå¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ ğŸ“„ .python-version              # Pythonç‰ˆæœ¬æŒ‡å®š
â”‚
â”œâ”€â”€ ğŸš€ launch_server.py             # æœåŠ¡å™¨å¯åŠ¨è„šæœ¬ (ä¸»å…¥å£)
â”œâ”€â”€ âš™ï¸ server_config.yaml           # é»˜è®¤é…ç½®æ–‡ä»¶  
â”œâ”€â”€ âš™ï¸ config_examples.yaml         # é…ç½®ç¤ºä¾‹æ–‡ä»¶
â”‚
â”œâ”€â”€ ğŸ§ª test_client.py               # HTTP API æµ‹è¯•å®¢æˆ·ç«¯
â”œâ”€â”€ ğŸ§ª test_openai_sdk_clean.py     # OpenAI SDK å…¼å®¹æµ‹è¯•
â”œâ”€â”€ ğŸ§ª sglang_example.py            # SGLang å‰ç«¯è¯­è¨€ç¤ºä¾‹
â”œâ”€â”€ ğŸ§ª sglang_example_optimized.py  # ä¼˜åŒ–ç‰ˆç¤ºä¾‹
â”‚
â”œâ”€â”€ ğŸ“± main.py                      # ç®€å•å¯åŠ¨å…¥å£
â””â”€â”€ ğŸ“‚ .venv/                       # è™šæ‹Ÿç¯å¢ƒ (uvåˆ›å»º)
```

## ğŸ” ç¤ºä¾‹å’Œæµ‹è¯•

### åŸºç¡€åŠŸèƒ½æµ‹è¯•

```bash
# 1. å¯åŠ¨æœåŠ¡å™¨
python launch_server.py --preset balanced

# 2. ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨å®Œæˆ (çœ‹åˆ° "Listening on http://0.0.0.0:30000")

# 3. åœ¨æ–°ç»ˆç«¯ä¸­æµ‹è¯•
python test_client.py --prompt "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
```

### é«˜çº§åŠŸèƒ½ç¤ºä¾‹

```bash
# SGLang å‰ç«¯è¯­è¨€ç¤ºä¾‹ (æ”¯æŒå¤šè½®å¯¹è¯ã€ç»“æ„åŒ–è¾“å‡ºç­‰)
python sglang_example.py

# OpenAI SDK å…¼å®¹æ€§æµ‹è¯•
python test_openai_sdk_clean.py

# ä¼˜åŒ–ç‰ˆç¤ºä¾‹ (æ‰¹é‡å¤„ç†ã€å¹¶å‘æµ‹è¯•ç­‰)
python sglang_example_optimized.py
```

## âš ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: å¯åŠ¨æ—¶æ˜¾ç¤º "CUDA out of memory"**
```bash
# è§£å†³æ–¹æ¡ˆ1: ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–é…ç½®
python launch_server.py --preset ultra_low_memory

# è§£å†³æ–¹æ¡ˆ2: å‡å°‘å†…å­˜åˆ†é…
python launch_server.py --preset balanced --mem-fraction-static 0.7

# è§£å†³æ–¹æ¡ˆ3: åˆ‡æ¢åˆ°æ›´å°çš„æ¨¡å‹
python launch_server.py --model-path /path/to/qwen3-4b --preset balanced
```

**Q: æ¨ç†é€Ÿåº¦å¾ˆæ…¢**
```bash
# è§£å†³æ–¹æ¡ˆ1: å¯ç”¨ç¼–è¯‘åŠ é€Ÿ
python launch_server.py --preset balanced --enable-torch-compile

# è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨æ›´å°‘é‡åŒ–
python launch_server.py --preset high_performance

# è§£å†³æ–¹æ¡ˆ3: æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†GPU
python -c "import torch; print(torch.cuda.is_available())"
```

**Q: API è¯·æ±‚è¶…æ—¶**
```bash
# å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œæ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
curl http://localhost:30000/health

# æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—
python launch_server.py --preset balanced  # æŸ¥çœ‹è¾“å‡ºæ—¥å¿—
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **é€‰æ‹©åˆé€‚çš„é‡åŒ–é…ç½®**ï¼šæ ¹æ®ç¡¬ä»¶é€‰æ‹©é¢„è®¾é…ç½®
2. **å¯ç”¨ç¼–è¯‘åŠ é€Ÿ**ï¼šæ·»åŠ  `--enable-torch-compile` å‚æ•°
3. **è°ƒæ•´å†…å­˜åˆ†é…**ï¼šæ ¹æ®å®é™…æ˜¾å­˜è°ƒæ•´ `--mem-fraction-static`
4. **ä½¿ç”¨å¤šGPU**ï¼šå¤§æ¨¡å‹å¯ä»¥ä½¿ç”¨ `--tp-size` è¿›è¡Œå¼ é‡å¹¶è¡Œ

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼

1. **æŠ¥å‘Šé—®é¢˜**ï¼šåœ¨ [Issues](https://github.com/wuyuVerse/sglang-qwen3-inference-/issues) ä¸­æŠ¥å‘Š bug æˆ–æå‡ºåŠŸèƒ½å»ºè®®
2. **æäº¤ä»£ç **ï¼šFork é¡¹ç›®ï¼Œåˆ›å»ºåˆ†æ”¯ï¼Œæäº¤ Pull Request
3. **å®Œå–„æ–‡æ¡£**ï¼šæ”¹è¿› READMEã€æ·»åŠ ç¤ºä¾‹æˆ–æ•™ç¨‹
4. **åˆ†äº«ç»éªŒ**ï¼šåˆ†äº«ä½¿ç”¨ç»éªŒå’Œæœ€ä½³å®è·µ

### å¼€å‘ç¯å¢ƒæ­å»º

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/wuyuVerse/sglang-qwen3-inference-.git
cd sglang-qwen3-inference

# å®‰è£…å¼€å‘ä¾èµ–
uv sync --dev

# è¿è¡Œæµ‹è¯•
python -m pytest tests/  # (å¦‚æœæœ‰æµ‹è¯•çš„è¯)
```

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [Apache 2.0 License](LICENSE) è®¸å¯è¯ã€‚

## ğŸ™ è‡´è°¢

- [SGLang](https://github.com/sgl-project/sglang) - å¼ºå¤§çš„ LLM æœåŠ¡æ¡†æ¶
- [Qwen](https://github.com/QwenLM/Qwen) - ä¼˜ç§€çš„å¼€æºè¯­è¨€æ¨¡å‹
- [TorchAO](https://github.com/pytorch/torchao) - é«˜æ•ˆçš„æ¨¡å‹é‡åŒ–åº“

## ğŸ“ è”ç³»æ–¹å¼

- **é¡¹ç›®ä¸»é¡µ**: https://github.com/wuyuVerse/sglang-qwen3-inference-
- **é—®é¢˜åé¦ˆ**: [GitHub Issues](https://github.com/wuyuVerse/sglang-qwen3-inference-/issues)
- **é‚®ç®±**: 1074275896@qq.com

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼â­**

</div>
